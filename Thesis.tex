\documentclass[a4paper,xcolor=dvipsnames, tikz, 12pt]{article}
%\documentclass{beamer}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry} % page settings
\usepackage{amsmath} % provides many mathematical environments & tools
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{textcomp}
\usepackage{pgf}
\usepackage{pgfgantt}
\usepackage{pgfplots}\pgfplotsset{compat=1.3}
\usepackage{setspace}
\usepackage[colorlinks,linkcolor=ForestGreen,citecolor=ForestGreen]{hyperref}
\usepackage[nameinlink, capitalize]{cleveref} %used for cross theorem references
\setlength{\parindent}{5mm}
\usepackage[english]{babel}
\usepackage{verbatim}

\usepackage{tikz}%used for examples for problems in earlier analysis

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{tabularx,ragged2e}%used for tables with long text in some columns
\newcolumntype{C}{>{\arraybackslash}X}%column type that supports long texts, i.e. text across multiple lines
\setlength\extrarowheight{3pt}

\crefname{lemma}{Lemma}{Lemmas}
\crefname{figure}{Figure}{Figure}

% !TeX spellcheck=en_GB

\newcommand{\R}{\mathbb{R}}
\newcommand{\nl}{\newline}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{purple}{#1}}

\newcommand{\crep}{\text{C{\scriptsize REP}}}
\newcommand{\size}{\text{\scriptsize SIZE}}
\newcommand{\optmig}{\text{\scriptsize OPT-MIG}}
\newcommand{\del}{\text{\scriptsize DEL}}
\newcommand{\ovr}{\text{\scriptsize OVR}}
\newcommand{\final}{\text{\scriptsize FINAL}}
\newcommand{\opt}{\text{O{\scriptsize PT}}}
\newcommand{\alg}{\text{A{\scriptsize LG}}}
\newcommand{\core}{\text{\scriptsize CORE}}
\newcommand{\halo}{\text{\scriptsize HALO}}
\newcommand{\req}{\text{\scriptsize REQ}}
\newcommand{\finalComps}{\text{\scriptsize FINAL-COMPS}}
\newcommand{\finalWeights}{\text{\scriptsize FINAL-WEIGHTS}}
\newcommand{\epoch}{\text{\scriptsize EPOCH}}
\newcommand{\delTime}{\text{\scriptsize DEL-TIME}}
\newcommand{\reqTime}{\text{\scriptsize TIME}}
\newcommand{\comps}{\text{\scriptsize COMPS}}
\newcommand{\onl}{\text{O{\scriptsize NL}}}

%commands for input sequence diagrams
\newcommand{\request}[3]{\draw (axis cs:#3,#1) -- node[left]{} (axis cs:#3,#2);}

\theoremstyle{definition}
\newtheorem{defi}{Definition}
\newtheorem{theo}[defi]{Theorem}
\newtheorem{lemma}[defi]{Lemma}
\newtheorem{bsp}[defi]{Example}
\newtheorem{coro}[defi]{Corollary}
\newtheorem{fact}[defi]{Fact}



%TODO find out journal or equivalent for some references


\begin{document}
	
	%title page
	\input{title/title_page}	
	
	\thispagestyle{empty}
	\begin{abstract}
		\ldots
	\end{abstract}
	
	\clearpage
	%plots for evaluation data, decomp tree with regular alpha
	% A - cesar mocfe
	% B - cesar nekbone First300k
	% C -
	% D - 
	%total cost
	\pgfplotstableread[row sep=\\,col sep=&]{
		database & decomposition & ParMetis & Static            \\
		A		 & 153223	 	 & 254994.20000  & 54919.400000 \\
		B		 & 313985.600000 & 263278.400000 & 57454.200000 \\
		C 		 & 295830.800000 & 173940.200000 & 14950.400000 \\
		%D		 & 100			 & 80 			 & 50			\\
	}\totalcostplot
	
	%communication cost
	\pgfplotstableread[row sep=\\,col sep=&]{
		database & decomposition & ParMetis & Static 			\\
		A		 & 55375.2		 & 93641.000000 & 48947.000000	\\
		B		 & 119710.400000 & 56231.600000 & 51519.000000	\\
		C		 & 110112.800000 & 59455.400000 & 9242.000000	\\
		%D 		 & 100			 & 80 			& 50			\\
	}\commcostplot
	
	%migration cost
	\pgfplotstableread[row sep=\\,col sep=&]{
		database & decomposition & ParMetis 	 & Static 		\\
		A		 & 97848		 & 161353.200000 & 5972.400000	\\
		B		 & 194275.200000 & 207046.800000 & 5935.200000	\\
		C		 & 185718.000000 & 114484.800000 & 5708.400000	\\
		%D		 & 100			 & 80 			 & 50			\\
	}\migcostplot
	
	%running time in ms
	\pgfplotstableread[row sep=\\,col sep=&]{
		database & decomposition 	& ParMetis		& Static		\\
		A		 & 102254			& 61374.6 		& 2588.399902	\\
		B		 & 622934.000000	& 32851.800781	& 2489.399902	\\
		C		 & 428089.812500	& 42036.601562	& 2460.399902	\\
		%D 		 & 100				& 80 			& 50			\\
	}\runtimeplot




{
	\hypersetup{linkcolor=black}
	\tableofcontents
	\thispagestyle{empty}
	\clearpage
}
\pagenumbering{arabic} 


	\section{Introduction}
	
	
	\section{Problem Definition}
	
	The task is to maintain a partitioning of a dynamic graph consisting of $n=k\cdot l$ nodes that communicate with each other into $k$ parts, each of size $l$ while minimizing both the cost due to communication and due to node migrations defined as follows. The communication cost is zero if both nodes are located on the same server at the time the request needs to be served and it is normalized to one if they are mapped to different servers. An algorithm may perform node migrations in order to change the mapping of nodes to servers prior to serving the communication request at time $t$. Such a move of one vertex incurs cost $\alpha>1$.
	
	More formally we are given $l$ servers $V_0,...,V_{l-1}$, each with capacity $k$ and an initial perfect mapping of $n=k\cdot l$ nodes to the $l$ servers, i.e. each server is assigned exactly $k$ nodes. An input sequence $\sigma=(u_1, v_1), (u_2, v_2),...(u_i,v_i),...$ describes the sequence of communication requests: the pair $(u_t, v_t)$ represents a communication request between the nodes $u_t$ and $v_t$ arriving at time $t$. At time $t$ the algorithm is allowed to perform node migrations at a cost of $\alpha>1$ per move. After the migration step the algorithm pays cost 1 if $u_t$ and $v_t$ are mapped to different servers and does not pay any cost otherwise. Note that an algorithm may also choose to perform no migrations at all.
	
	We are in the realm of competitive analysis and as a result we compare an online algorithm \onl{} to the optimal offline algorithm \opt{}. \onl{} only learns of the requests in the input sequence $\sigma$ as they happen and as a result only knows about the partial sequence $(u_1,v_1),...,(u_t,v_t)$ at time $t$ whereas \opt{} has perfect knowledge of the complete sequence $\sigma$ at all times.
	
	The goal is to design an online algorithm \onl{} with a good competitive ratio with regard to \opt{} defined as follows.
	
	An online algorithm \onl{} is $\rho-competitive$ if there exists a constant $\beta$ such that 
	\begin{align*}
	\onl{}(\sigma)\leq\rho\cdot \opt{}(\sigma)+\beta\:\forall \sigma
	\end{align*} 
	where $\onl{}(\sigma)$ and $\opt{}(\sigma)$ denote the cost of serving input sequence $\sigma$ of \onl{} and \opt{} respectively.
	
	Often we allow the online algorithm to use larger capacities per server. In this case we speak of an \textit{augmentation} of $\delta$ in the case where the online algorithm is allowed to assign $\delta\times n/k$ nodes to each server where $\delta>1$. This augmented online algorithm is than compared with the optimal offline algorithm \opt{} which is not allowed to use any augmentation.
	
	\section{Related Work}
	\subsection{Dynamic Balanced RePartitioning}
	%TODO adapt if current problems cant be resolved
	Avin et al.\cite{Avin2015a} initiated the study of the online variant of the Balanced RePartitioning (BRP) problem that is the topic of this thesis. We discovered flaws in their competitive analysis which makes us question their results. These flaws are discussed in greater detail in \cref{flawsSection}. %While they gave a competitive algorithm, their approach could not be used for an actual implementation as they did not provide a polynomial-time implementation. In this thesis we will show an adaptation of their algorithm \crep{} which both preserves the initial competitive ratio and can also be performed in polynomial time.\nl
	
	\subsection{Restricted Variant of Balanced RePartitioning}
	Restricted variants of the Balanced RePartitioning problem have also been studied. Here one assumes certain restrictions of the input sequence $\sigma$ and then studies online algorithms for these cases. 
	
	Avin, Cohen, Parham and Schmid (\cite{Avin2018}) study one such case: the authors assume that an adversary provides requests according to a fixed distribution of which the optimal algorithm \opt{} has knowledge while an online algorithm that is compared with \opt{} has not. Further they restrict the communication pattern to form a ring-like pattern, i.e. for the case of $n$ nodes $0,...,n-1$ only requests $r$ of the form $r=\{i \mod n, (i+1)\mod n\}$ are allowed. For this case they present a competitive online algorithm which achieves a competitive ratio of $O(\log n)$ with high probability.	
	
	Henzinger, Neumann and Schmid (\cite{Henzinger2019}) studies a special \textit{learning variant} of the Dynamic Balanced Graph Partitioning problem specified above. In this version it is assumed that the input sequence $\sigma$ eventually reveals a perfect balanced partitioning of the $n$ nodes into $l$ parts of size $k$ such that the edge cut is zero. In this case the communication patterns reveal connected components of the communication graph of which each forms one of the partitions. Algorithms are tasked to \textit{learn} this partition and to eventually collocate nodes according to the partition while minimizing communication and migration costs.
	
	\cite{Henzinger2019} present an algorithm for the case where the number of servers is $l=2$ that achieves a competitive ratio of $O((\log n)/\epsilon)$ with augmentation $\epsilon$, i.e. each server has capacity $(1+\epsilon)n/2$ for $\epsilon\in(0,1)$.
	
	For the general case of $l$ servers of capacity $(1+\epsilon)n/l$ the authors construct an exponential-time algorithm that achieves a competitive ratio of $O((l\log n \log l)/\epsilon)$ for $\epsilon\in(0,1/2)$ and also provide a distributed version.
	Additionally the authors describe a polynomial-time $O((l^2\log n\log l)/\epsilon^2)$-competitive algorithm for the case with general $l$, servers of capacity $(1+\epsilon)n/l$ and $\epsilon\in(0,1/2)$.
	
	It is important to stress that the assumption that the requests reveal a perfect partitioning of the communication nodes is not applicable for most practical applications and thus it is important to study the general BRP problem without restricting $\sigma$.
	
	\section{Algorithmic Ideas}
	In this section we describe different solution approaches to the Dynamic Balanced Graph Partitioning problem. Both methods share a similar concept at their core: a second-order partitioning of the communication nodes into \textit{communication components} which represent node-induced sub-graphs of the original communication graph given by the requests from the input sequence $\sigma$. As more requests from $\sigma$ are revealed to the algorithms they merge the corresponding components once they are suitably connected and relocate the nodes of the new component in such a way that all the nodes of a component are always located on the same server.
	We first describe this general approach that is common to both algorithms and then address the specific differences and analysis ideas.
	
	More formally, initially each node forms a singleton component, but as the input sequence $\sigma$ is revealed to the algorithms new communication patterns unfold. The algorithm keeps track of these patterns by maintaining a graph in which the nodes represent the actual communication nodes and the weighted edges represent the number of communication requests between nodes that were part of different components at the time of the request, i.e. for edge $e=\{u,v\}$, $w(e)$ represents the number of paid communication requests between $u$ and $v$. We say that a communication request between nodes $u$ and $v$ is \textit{paid} if the nodes are located o different servers at the time of the request.
	
	Both algorithms merge a set $S$ of components into a new component $C$ if the connectivity of the component graph induced by the components in $S$ is at least $\alpha$. After each edge insertion the algorithm checks whether there exists a new component set $S$ with $|S|>1$ which fulfills this requirement.
	
	If after any request and the insertion of the resulting edge the algorithm discovers a new subset $S$ of nodes whose induced subgraph has connectivity at least $\alpha$ and which is of cardinality at most $k$ it merges the components that form this set into one new component and collocates all the nodes in the resulting set on a single server. If the resulting component has size at least $2/\epsilon$ the algorithm reserves additional space $\min\{\epsilon\cdot|C|,k\}$, otherwise the reservation is zero. 
	
	If the subset has cardinality greater than $k$ the resulting component is deleted. The definition of this deletion process is the main differentiating factor between our algorithms.
	
	\subsection{CORE-DEL}
	
	%TODO maybe write more about connectivity vs density
	 The first one resets edges which are \textit{contained} in the deleted component, i.e. all edges $e=\{u,v\}$ are reset to zero if $u$ and $v$ were contained in component $C$ at the time of its deletion. This approach resembles the one suggested by Avin et al. (\cite{Avin2015a}).
	 
	 \subsection{ADJ-DEL}
	
	The second algorithm resets all the edges contained in the deleted component $C$ but also resets the weights of edges \textit{adjacent} to $C$, i.e. all edges $e=\{u,v\}$ are reset to zero if $u$ or $v$ were contained in component $C$ at the time of its deletion. This second version is very similar to the algorithm proposed by Avin et al. in \cite{Avin2015}.
	
	The collocation of such component sets of at most $k$ individual communication nodes is always possible without moving a node in not in $C$ due to the allowed augmentation of $2+\epsilon$. This guarantees by an averaging argument that there is almost at least one cluster with capacity at least $k$ which a newly merged component can be moved to. We have also included a pseudocode description of this general outline of the algorithms in \cref{highLevelAlg}. Please note that the subroutine \textit{delete(Y)} of a component set $Y$ is different for each of the algorithms.
	
	%TODO include reservation in algorithm description
	\begin{algorithm}
		\caption{DynamicDecomp}
		\label{highLevelAlg}
		\begin{algorithmic}
			\STATE Initialize an empty graph on n nodes
			\STATE turn each of the n nodes into a singleton component
			\FORALL{$r=\{u,v\}\in\sigma$}
			
			\IF{$comp(v)\neq comp(u)$}
			\STATE $w(\{u,v\})\gets w(\{u,v\})+1$
			\ENDIF
			\IF{$\exists$ component set $X$ with connectivity at least $\alpha$ and $|X|>1$ and $nodes(X)\leq k$}
			\STATE merge(X)
			\ENDIF
			\IF{$\exists$ component set $Y$ with connectivity at least $\alpha$ and $nodes(Y)> k$}
			\STATE delete(Y)
			\ENDIF	
			
			\ENDFOR
			
		\end{algorithmic}
	\end{algorithm}
	
	The idea for the analysis of both approaches is to account cost to \opt{} for each component $C$ that is deleted by one of the algorithms.
	
	
	\section{Problems With Analysis in Previous Work}
	\label{flawsSection}
	In this section we point out problems in the analysis in two versions of \cite{Avin2015} which share a similar approach to ours.
	
	The problem in the analysis of \cite{Avin2015a} arises due to their usage of the concept of $F(c)$: This contains only edges incident to components from $S(c)$ that arrived after the involved components from $S(c)$ were created by \crep{}.In the analysis it is assumed that only those edges that are contained in $F(c)$ can contribute to the creation of component $c$. As we will show that is not the case and as a result it is very challenging to separate the costs of \opt{} that are due to the requests that actually led to the creation of $c$.
	
	In order for the approach in \cite{Avin2015a} to work the different sets $F(c)$ that occur as \crep{} handles input $\sigma$ need to form a partition of all requests such that each request can be mapped to one unique set $F(c)$. Only in this case it is possible to lower bound the cost of \crep{} via the requests in $\bigcup_{c\in\del{}}F(c)$.\
	
	\cref{exNewCrep} shows an example sequence of requests for which this approach does not work. The diagram shows horizontal lines, each representing one of the vertices. A vertical line represents a communication request between its end points. For example the sequence shown contains a communication request between nodes 1 and 2 at time $t=5$.
	
	In this sequence the first two requests are not contained in $F(c)$ for any component $c$ as nodes 3 and 4 were part of other components that were eventually deleted by \crep{} before finally being merged at time $t=15$.\nl
	
	\begin{figure}
		\begin{tikzpicture}
		\begin{axis}[xmin=0, xmax=20, ymin=0, ymax=4.5, axis lines=left, xlabel= time, ylabel= nodes]
		%nodes
		\addplot[domain=0:20, dashed] {1};
		\addplot[domain=0:20, dashed] {2};
		\addplot[domain=0:20, dashed] {3};
		\addplot[domain=0:20, dashed] {4};
		
		%requests
		\request{4}{3}{1}
		\request{4}{3}{2}
		\request{1}{2}{3}
		\request{1}{2}{4}
		\request{1}{2}{5}
		\request{2}{3}{6}
		\request{2}{3}{7}
		\request{2}{3}{8}
		\request{1}{2}{9}
		\request{1}{2}{10}
		\request{1}{2}{11}
		\request{2}{4}{12}
		\request{2}{4}{13}
		\request{2}{4}{14}
		\request{4}{3}{15}
		\end{axis}
		
		
		\end{tikzpicture}
		\caption{problem in the new version}\label{exNewCrep}	
	\end{figure}
	
	In an older version (\cite{Avin2015}) of the paper mentioned above we have also discovered some problems. In this version a similar component structure is maintained and merges are performed similarly to our approach as upon the deletion of a component the algorithm not only deletes inner edges but also those that leave the component, i.e. all edges $e={u,v}$ are deleted where $u$ or $v$ are inside the component. However the additional deletions of edges that are leaving the component are not accounted for in the analysis. We show that this approach may lead to scenarios where the amount of edges that are deleted that leave the component may greatly exceed the amount of inner edges. We show in \cref{analysisSection} how the analysis can be adapted while preserving the competitive ratio.
	
	\cref{exOldCrep} illustrates an example of the problems mentioned above for this older version of \crep{}. In the illustration it is assumed that $\alpha=3$ and $k=3$. In this case the input sequence leads \crep{} to first merge nodes 1 and 2 at time $t=3$ and then to add node 3 to the resulting component at time $t= 6$. The next two requests do not quite lead to the merge of the node 4 with the component. Instead a series of requests follows where node 1 communicates with other nodes that are outside of its component without any merges. Note that this sequence can be extended until node $1$ has communicated with every node except from the nodes $1,2,3,4$. Finally the first 4 nodes are merged at time $t=15$ at which point the resulting component is deleted as well as \textit{all} edges in the sequence. 


\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[xmin=0, xmax=20, ymin=0, ymax=7.5, axis lines=left, xlabel = time, ylabel = nodes]
	%nodes
	\addplot[domain=0:20, dashed] {1};
	\addplot[domain=0:20, dashed] {2};
	\addplot[domain=0:20, dashed] {3};
	\addplot[domain=0:20, dashed] {4};
	\addplot[domain=0:20, dashed] {5};
	\addplot[domain=0:20, dashed] {6};
	\addplot[domain=0:20, dashed] {7};
	
	%requests
	\request{1}{2}{1}
	\request{1}{2}{2}
	\request{1}{2}{3}
	\request{2}{3}{4}
	\request{2}{3}{5}
	\request{2}{3}{6}
	\request{3}{4}{7}
	\request{3}{4}{8}
	\request{1}{5}{9}
	\request{1}{5}{10}	
	\request{1}{6}{11}
	\request{1}{6}{12}
	\request{1}{7}{13}
	\request{1}{7}{14}
	\request{1}{3}{15}
	
	\end{axis}
	
	
	\end{tikzpicture}
	\caption{problem in the older version}\label{exOldCrep}
	
\end{figure}
	
	
	
	\section{Algorithm Overview}
	In this section we give an overview of our algorithm \crep{} which builds upon the ideas of \cite{Avin2015} and \cite{Avin2015a}. Implementation details will be discussed in greater detail in \cref{implDetSection}.
	
	\subsection{Overview}
	The algorithm maintains a second-order partitioning of the nodes into \textit{communication components} which are sets of nodes that communicated frequently. As more requests from the input sequence $\sigma$ are revealed to the algorithm the components grow in size until the algorithm discovers a component that is too large and hence decides to delete it.
	
	

\section{Analysis}
We analyse the competitive ratio of \crep{} with augmentation $(2+\epsilon)$ and show in \cref{comp_ratio_theo} that \crep{} is $O(k\log k)$-competitive.
\label{analysisSection}

\subsection{Algorithm Definitions}
We begin our analysis by introducing two general definitions that we will use throughout the analysis.

\begin{defi}
	Define for any subset S of components $w(S)$ as the total weight of all edges between nodes of S.
\end{defi}

Note that such an edge can only have positive weight if its endpoints are in different components.

\begin{defi}
	Let a set of components of size at least 2 and of connectivity $\alpha$ be a \textit{mergeable} component set.	
\end{defi}


\subsection{Structural Properties}


\textbf{Note:} These properties are changed to use the connectivity based approach which generally simplifies them, but guarantees slightly less minimum edge weight within mergeable component sets.
\begin{defi}
	An $\alpha$\textit{-connected component} is a maximal set of vertices that is $\alpha$-connected.
\end{defi}

\begin{lemma}
	At any time $t$ after \crep{} performed its merge and delete actions all subsets $S$ of components with $|S|>1$ have connectivity less than $\alpha$, i.e. there exist no mergeable component sets after \crep{} performed its merges.
\end{lemma}

\textit{Proof.} We proof the lemma by an induction on steps. The lemma holds trivially at time 0.

Now assume that at some time $t>0$ the lemma does not hold, i.e. there is a subset $S$ of components with connectivity at least $\alpha$ and $|S|>1$. We may assume that $t$ is the earliest time for which $S$ has connectivity $\alpha$.

Then the incrementation of the weight of edge $e$ at time $t$ raised the connectivity of S, but S was not merged into a new $\alpha$-connected component C. if no new component was created at time $t$ then we arrive at a contradiction as \crep{} always merges if there exists a mergeable component set.

Now assume that a component $C$ was created at time t. This means that $C$ must also contain the endpoints of $e$. But then the conjunction of C and S forms an even larger subset of components with connectivity at least $\alpha$ which is a contradiction to the maximality of C and S.

\begin{lemma}
	\label{mergeable_lemma}
	Fix any time t and consider weights right after they were updated by \crep{} but before any merge or delete actions. Then all subsets $S$ of components with $|S|>1$ have connectivity at most $\alpha$ and a mergeable component set S has connectivity exactly $\alpha$.
\end{lemma}

\textit{Proof.} This lemma follows directly from lemma 2 as connectivities can only increase by at most 1 at each time t.

\begin{lemma}
	\label{cut_lemma}
	The weight between the components of a component subset S of connectivity $\alpha$ is at least $|S|/2 \cdot alpha$.
\end{lemma}
\textit{Proof.} Consider the sum of the weighted degrees of all components:
\begin{align*}
\sum_{c\in S}deg_S(c)=2\sum_{e\in S}w(e)
\end{align*}
The equality follows as the left sum counts each edge twice, once for each endpoint.
Now consider the fact that each component must have degree at least $\alpha$ with respect to the edges in S as S has connectivity $\alpha$ and hence the lemma follows.
%TODO rephrase like the lemma in the original analysis, with F(c)

\begin{lemma}
	%TODO rephrase this such that it is clear that this only holds during the normal algorithm flow, i.e. if merges are performed as soon as possible as otherwise we might encounter subsets of cardinality greater than 1 with connectivity greater than alpha
	\label{cut_lemma_upper}
	The weight between the components of a component subset $S$ of connectivity $\alpha$ is at most $(|S|-1)\cdot\alpha$.
\end{lemma}
\textit{Proof.} We iteratively partition $S$ into subsets via minimum cuts with regard to edge weight, i.e. we consider a minimum edge cut of $S$ which partitions $S$ into the subsets $S_1$ and $S_2$ and iteratively partition the resulting sets until all sets contain only one component each. As this required at most $|S|-1$ cuts of value at most $\alpha$ the lemma follows.

\subsection{Upper Bound On \crep{}}

We define the set $\del(\sigma)$ as the set of components that were deleted by \crep{} during its execution given the input sequence $\sigma$.

We define the following notions for a component $C\in\del{}(\sigma)$, i.e. the subgraph induced by the nodes of $C$ has connectivity at least $\alpha$ and $C$ consists of more than $k$ nodes:

Let $\epoch{}(C)$ denote the (node, time) pairs of nodes in $C$ starting at the time after the time $\tau(node)$ when $node$ was last turned into a singleton component, i.e. $\epoch{}(C)=\bigcup_{n\in nodes(C)}\{n\}\times\{\tau(n)+1,...,\tau(C)\}$. Note that for $C\in\del(\sigma)$, $\tau(C)$ denotes both the time of the creation as well as the time of deletion of $C$. We can use this definition of a component epoch $\epoch(C)$ to uniquely assign each node to a deleted component $C$ at each point in time $t$ (except for nodes in components that persist until the end of sequence $\sigma$).

We assign all requests to $\epoch(C)$ whose corresponding requests are deleted because of the deletion of component $C$ and call the set of those requests $\req(C)$.
We split the requests from $\req{}(C)$ into two sets: $\core(C)$ contains all requests for which both nodes have already been assigned to $C$ at the time of the request, i.e. 
\begin{align*}
\core(C)=\{r=\{u,v\}\in\sigma| (u,\reqTime(r))\in \epoch(C)\text{ and } (v, \reqTime(r))\in \epoch(C)\}.
\end{align*}
These are the requests that led to the creation of component $C$. 
$\halo(C)$ contains all requests from $\req(C)$ for which exactly one end point was associated with $C$ at the time of the request. Note that this means that $\halo(C)=\req(C)\backslash \core(C)$.

We start the analysis by bounding the communication cost of \crep{} that is due to serving requests from $\core(C)$ for $C\in\del{}(\sigma)$.

\begin{lemma}
	\label{core_comm_upper}
	With augmentation $2+\epsilon$, \crep{} pays at most communication cost $|C|\cdot\alpha$ for requests in $\core(C)$ where $C\in\del{}(\sigma)$.
\end{lemma}

\textit{Proof.} The lemma follows directly from \cref{cut_lemma_upper} due to the fact that component sets of connectivity $\alpha$ get merged immediately by \crep{} as shown in \cref{mergeable_lemma}.\qed\nl

We define $\finalWeights{}(\sigma)$ as the total amount of edge weight between the components $\finalComps{}(\sigma)$ which are present after the execution of \crep{} given input sequence $\sigma$.

Together with the fact that \crep{} pays for all requests in $\halo(C)$ for deleted components $C$ we use these definitions as well as the previous lemma to bound the total communication cost of \crep{} in the following lemma.

\begin{lemma}
	\label{crep_req_bound}
	The cost of serving communication requests that \crep{} has to pay, denoted by $\crep{}^{req}(\sigma)$ given input sequence $\sigma$ is bounded by \nl$\crep{}^{req}(\sigma)\leq\sum_{C\in\del{}(\sigma)}(|C|\cdot\alpha+|\halo(C)|)+\sum_{C\in \finalComps(\sigma)}|C|\cdot\alpha+\finalWeights(\sigma)$.
\end{lemma}

\textit{Proof.} The number of communication requests that led to the creation of a component $C$ is bounded by $|C|\cdot\alpha$ due to \cref{cut_lemma_upper}.
If component $C$ was deleted by \crep{} then also the requests from $\halo(C)$ were deleted. All other edge weights were not changed. The remaining communication requests that have not been accounted for so far have either led to the creation of component $C\in\finalComps(\sigma)$ and are hence also bounded by $|C|\cdot\alpha$ or have not let \crep{} to any merge and are hence contained in $\finalComps(\sigma)$. This concludes the proof.\qed\nl

We continue our analysis by bounding the migration cost of \crep{} in the following lemma.
%TODO specify base of the logarithm
\begin{lemma}
	\label{crep_mig_bound}
	With augmentation $2+\epsilon$, \crep{} pays at most migration costs of\nl $\sum_{C\in\del{}(\sigma)\cup\finalComps(\sigma)}|C|\cdot((2/\epsilon+1)+\log k)\cdot\alpha$.
\end{lemma}

\textit{Proof.} First note that \crep{} only performs migrations when it merges components.
We fix a component $C\in\del{}(\sigma)\cup \finalComps(\sigma)$ and bound the number of times each node of $C$ is moved as \crep{} processes the requests that led to the creation of $C$. 

As \crep{} only reserves additional space $\lfloor\epsilon\cdot|B|\rfloor$ for components of size at least $2/\epsilon$ for each component $B$ and only moves component $B$ when a merge results in a component of size more than $(1+\epsilon)*|B|$ each node of $C$ is moved at most
$(2/\epsilon+1)+\log k$ times. Summing over all nodes in $C$ that were actually moved by \crep{} bounds the number of migrations by $|C|\cdot((2/\epsilon+1) \log k)$ as components get deleted without migrations once they contain more than $k$ nodes. This leads to the desired bound on the migration costs as each node migration incurs cost $\alpha$ to \crep{}.\qed\nl

Finally we summarize our results in the following lemma.

\begin{lemma}
	\label{crep_upper_bound}
	With augmentation $2+\epsilon$, \crep{} pays at most total cost\nl $2\cdot\sum_{C\in\del{}(\sigma)\cup\finalComps(\sigma)}|C|\cdot((2/\epsilon+1)+\log k)\cdot\alpha+\sum_{C\in\del{}(\sigma)}|\halo(C)|+\finalWeights(\sigma)$.
\end{lemma}
%TODO make this proof more understandable by starting with a more genral statement and plugging in the previous lemmas
\textit{Proof.} The lemma follows by summing the results from \cref{crep_req_bound} and \cref{crep_mig_bound}. 

\subsection{Lower Bound on \opt{}}
\begin{lemma}
	\label{opt_lower_bound}
	The cost of the solution of \opt{} given input sequence $\sigma$ is bounded by
	\begin{align*}
	\opt{}(\sigma)\geq1/2\cdot\sum_{C\in\del{}(\sigma)}|C|/k\cdot\alpha+|\halo(C)|/k +|P|.
	\end{align*}
	where $P$ denotes the set of edges from $\bigcup_{C\in\del{}(\sigma)}\halo{}(C)$ that both \crep{} and \opt{} pay for.
\end{lemma}

\textit{Proof.} First we define the term \textit{offline interval} of a node $v$ to be the time between two migrations of $v$ in the solution of \opt{}. More specifically an offline interval of node $v$ either starts at time zero (if it is the first offline interval of $v$) or after a migration of $v$ and ends with the next migration of node $v$ that \opt{} performs.

Furthermore we say that an offline interval is contained in the epoch $\epoch(C)$ of a component $C\in\del{}(\sigma)$ if it ends before the time $\tau(C)$. Note that $\tau(C)$ is both the time of the creation of $C$ in the solution of \crep{} as well as the time of its deletion as $C\in\del{}(\sigma)$.

We assign a request $r$ involving the node $v$ to an offline interval of $v$ if it is both the first offline interval of one of the end points of $r$ that ends and if the offline interval ends before the deletion of the edge representing $r$ due to a component deletion.

The requests that are not assigned to any offline interval are then those which are deleted due to the deletion of a component and those that are present at the end of the execution of \crep{}. For the sake of showing the lower bound from the lemma we choose to ignore the latter.

We start by bounding the total edge weight (the total number of requests) we assign to any one offline interval when limiting ourselves to requests from\nl $\bigcup_{C\in\del{}(\sigma)}\halo{}(C)$ which \crep{} pays for but \opt{} does not. We denote the requests in question, i.e. those from $\bigcup_{C\in\del{}(\sigma)}\halo{}(C)$ which \opt{} does not pay for by $N$.
As we only examine the amount of such edges assigned to one offline interval we observe that none of the nodes involved in the assigned requests are moved by \opt{} during the offline interval, hence all the requests in question involve only nodes that \opt{} has placed on the same server as $v$ during the offline interval. The number of such nodes is hence limited by the server capacity $k$. As we only examine requests from $\bigcup_{C\in\del{}(\sigma)}\halo{}(C)$ we know that none of these requests have led \crep{} to perform any merges, hence there were at most $\alpha$ requests between $v$ and any one of the other nodes on its server. This bounds the number of requests assigned to the online interval by $k\cdot\alpha$.
Let $I$ denote the set of requests we have assigned to offline intervals in this way.

For the following part of the proof we fix an arbitrary component $C\in\del{}(\sigma)$. Let $R$ denote the set of requests from $\halo(C)$ that were not assigned to any offline interval. This means that the nodes involved in requests from $R$ were not moved during the processing of requests from $R$ until the time of deletion of $C$.

The number of nodes contained in $C$ or connected to $C$ via edges representing requests from $R$ is at least $|C|+R/\alpha$ since requests from $R$ have not led \crep{} to perform any migrations. Because of this fact \opt{} must have placed those nodes on at least $\frac{|C|+R/\alpha}{k}$ different servers. As \opt{} does not pay for any requests from $R$ it follows that \opt{} must have placed the nodes from $C$ in $\frac{|C|+R/\alpha}{k}$ different servers.

We first examine the case in which \opt{} does not move any nodes from $C$ during $\epoch(C)$. In this case \opt{} must partition a graph containing the nodes from $C$ which are connected via edges representing the requests from $\core(C)$into migrations. Because of this fact \opt{} must have placed those nodes on at least $\frac{|C|+R/\alpha}{k}$ parts. As \crep{} merged component $C$ this graph is $\alpha$-connected and hence \cref{cut_lemma} gives that \opt{} has to cut at least edges of total weight migrations. Because of this fact \opt{} must have placed those nodes on at least $\frac{|C|+R/\alpha}{k}\cdot\alpha=|C|/k\cdot\alpha+R/k$.

For the more general case in which \opt{} may perform node migrations during $\epoch(C)$ we adapt the graph construction from above as follows: we add a vertex representing each (node, time) pair from $\epoch(C)$.
We connect each (node, time) pair $p$ with edges of weight $\alpha$ to the pairs of the same node that represent the time step directly before and directly after $p$ (if they exist in the graph). These edges represent the fact that \opt{} may choose to migrate a node between any two time steps in $\epoch(C)$.
Additionally we add an edge of weight one for each request $r=\{u,v\}$ from $\core(C)$ by connecting the nodes in the graph that represent the pairs $(u,t)$ and $(v,t)$, respectively. \opt{} once again has to partition this graph into $\frac{|C|+R/\alpha}{k}$ parts.

Note that we only added edges of weight $\alpha$ to the graph and hence this graph is also $\alpha$-connected. We conclude that once again \opt{} has to cut edges of weight at least $\frac{|C|+R/\alpha}{k}\cdot\alpha=|C|/k\cdot\alpha+R/k$.

Finally we need to account for the fact that the migrations of nodes from $C$ that \opt{} performs also end offline intervals and might hence be accounted for twice in our analysis up to this point:
\begin{align*}
2\cdot\opt(\sigma)\geq\sum_{(C\in\del{}(\sigma)}|C|/k\cdot\alpha+R(C)/k+I/k\cdot\alpha=\sum_{(C\in\del{}(\sigma)}|C|/k\cdot\alpha+\halo(C)/k
\end{align*}
where the last equality follows from the fact that 
%TODO think about whether this is actually true, likely P is missing here 
$\bigcup_{C\in\del(\sigma)}R(C)\cup I=\bigcup_{C\in\del(\sigma)}halo(C)$ as the different $R(C)$ as well as $I$ are disjoint.
Hence the cost of \opt{} is at least $1/2\cdot\sum_{(C\in\del{}(\sigma)}|C|/k\cdot\alpha+\halo(C)/k+|P|$ as \opt{} pays for requests from $P$ by the definition of $P$.



\begin{comment}
	

\subsection{Lower Bound on \opt{}}
We split the analysis of the cost \opt{} has to pay into two parts. First we examine the cost incurred to \opt{} due to requests that led \crep{} to create (and delete) a component $C\in\del{}(\sigma)$, i.e. those from $\core(C)$. Then we provide a lower bound on the cost incurred to \opt{} due to requests from $\halo(C)$ for $C\in\del{}(\sigma)$.

\begin{lemma}
	\opt{} pays at least $\lceil\frac{|C|}{k}\rceil\cdot\alpha/2$ due to requests from $\core(C)$ for any $C\in\del{}(\sigma)$.
\end{lemma}

\textit{Proof.} We fix $C\in\del{}(\sigma)$ arbitrarily and observe that any solution that does not move any vertices of $C$ during the epoch of $C$ incurs cost at least $\lceil\frac{|C|}{k}\rceil\cdot\alpha/2$ as the nodes of $C$ need to be distributed on at least $\lceil\frac{|C|}{k}\rceil$ servers. \cref{cut_lemma} gives a suitable lower bound on the weight between these servers and hence also on the number of communication requests an algorithm which chooses this static configuration must pay for.

We show that it is not possible for \opt{} to improve upon this static solution via migrations. Observe that a migration involving only one node that is mapped to a server from the static configuration may only increase the communication costs for requests in $\core(C)$ as it increases the number of partitions in the result of \cref{cut_lemma}. A migration between two servers used in the static configuration may only decrease the weight between the two servers involved and incurs cost $2\alpha$ to \opt{}. Note that the proof of  \cref{cut_lemma} only requires the weight between one component set S and all the other component sets that together form a mergeable set to be at least $\alpha$. If \opt{} now performs a swap of two nodes we can attribute cost $2\alpha$ to \opt{} for the connection between these two servers which may only increase the cost we can attribute to \opt{}. Hence the lemma follows.\qed\nl

\begin{lemma}
	\opt{} pays at least $\sum_{C\in\del{}(\sigma)}|\halo(C)|/k$ for serving requests from\nl $\bigcup_{C\in\del{}(\sigma)}\halo(C)=:H$.
\end{lemma}

\textit{Proof.} First note that we may ignore requests from $H$ that both \crep{} and \opt{} pay for as including these may only give \crep{} an advantage in our analysis.

In the following we only consider those requests from $H$ that \crep{} pays for while \opt{} does not. We relate the number of those requests that \opt{} does not pay to the number of migrations \opt{} performs by assigning each request from $H$ to the next migration performed by \opt{} that moves a node involved in the request.

We now bound the number of requests from $H$ \opt{} may be able to serve without cost by assigning nodes to servers and performing migrations optimally. Consider all requests from $H$ that happen between nodes that \opt{} has placed on the same server between \opt{} migrations that involve nodes from that server. As the requests from $H$ have not led \crep{} to perform a migration there can be at most $(k-1)\cdot\alpha$ such requests before \opt{} changes the nodes mapped to the server by performing a migration. As this migration incurs cost $2\alpha$ to \opt{} we conclude that \opt{} may only improve its cost by a factor of $k$ over the cost paid by \crep{} for serving requests from $H$.\qed\nl

We obtain the final lower bound on the cost of \opt{} by combining the results of the previous two lemmas. Note that migrations may allow \opt{} to both reduce the cost associated with $\core(C)$ and those from $\halo(B)$ for $B,C\in\del(\sigma)$. Hence we only account half of the cost of \opt{} and conclude our analysis on the lower bound on \opt{} with the following lemma.

\begin{lemma}
	%\label{opt_lower_bound}
	$\opt{}(\sigma)\geq1/2\sum_{C\in\del{}(\sigma)}(\lceil\frac{|C|}{k}\rceil\cdot\alpha/2+|\halo(C)|/k)$.
\end{lemma}
\textit{Proof.} The proof follows from the two previous lemmas and the thoughts on these.
\end{comment}

\subsection{Competitive Ratio}
In this section we combine the results of \cref{crep_upper_bound} and \cref{opt_lower_bound} to obtain the following theorem.

\begin{theo}
	\label{comp_ratio_theo}
	With augmentation $(2+\epsilon)$ the competitive ratio of \crep{} is in $O(k \log k)$.
\end{theo}

\textit{Proof.} We arbitrarily fix an input sequence $\sigma$ and use our previous results to bound the competitive ratio of \crep{}. We define $\comps(\sigma):=\del{}(\sigma)\cup\finalComps(\sigma)$ in order to improve readability. Let $P$ denote the set of edges from $\bigcup_{C\in\del{}(\sigma)}\halo{}(C)$ that both \crep{} and \opt{} pay for.

\begin{align*}
\frac{\crep{}(\sigma)}{\opt(\sigma)} &\leq\frac{2\cdot\sum_{C\in\comps(\sigma)}|C|\cdot((2/\epsilon+1)+\log k)\cdot\alpha+\sum_{C\in\del{}(\sigma)}|\halo(C)|+\finalWeights(\sigma)}{1/2\cdot\sum_{C\in\del{}(\sigma)}|C|/k\cdot\alpha+|\halo(C)|/k +|P|}\\
&\leq k \log k\frac{2\cdot\sum_{C\in\del(\sigma)}|C|\cdot(2/\epsilon+1) \cdot\alpha+\sum_{C\in\del{}(\sigma)}|\halo(C)|}{1/2\sum_{(C\in\del{}(\sigma)}|C|\cdot\alpha/2+|\halo(C)|)}+\beta\\
& = O(k \log k) +\beta
\end{align*}

\noindent where $\beta=\sum_{C\in \finalComps(\sigma)}|C|\cdot((2/\epsilon+1)+\log k)\cdot\alpha+\finalWeights(\sigma)$.

To obtain the bound on $\beta$ we observe that the components in $\finalComps(\sigma)$ each are of size at most $k$ since they were not deleted by \crep{}. This allows us to derive to bound $\sum_{C\in \finalComps(\sigma)}|C|\cdot((2/\epsilon+1)+\log k)\leq l \cdot k \cdot ((2/\epsilon+1)+\log k)$. Since at the end of the execution of \crep{} there can be at most $k\cdot l$ components, \cref{cut_lemma_upper} allows us to bound $\finalWeights(\sigma)$ by $k\cdot l \cdot\alpha$. Hence we conclude that $\beta\leq l\cdot k \cdot
((2/\epsilon+1)+log k)\cdot\alpha+k\cdot l\cdot \alpha\in O(k \log k)$.\qed


\section{Implementation Details}
\label{implDetSection}


\subsection{Algorithm Pseudocode}

\begin{algorithm}
	\caption{insertAndUpdate(a,b)}
	\label{insertAndUpdate}
	\begin{algorithmic}
		\IF {comp[a] == comp[b]}
		\STATE return
		\ENDIF
		\STATE addEdge($a,b$)
		\STATE updateDecomposition$(a,b)$
		\STATE del$\leftarrow$ updateMapping$(alphaConnectedComponents)$
		\STATE delComponents$(del)$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{updateDecomposition(a,b)}
	\label{updateDecomposition}
	\begin{algorithmic}
		\STATE q$\leftarrow$ findSmallestSubgraph$(a,b)$
		\WHILE{q not empty}
		\STATE current$\leftarrow$ q.popFront()
		\IF{res.connectivity==alpha}
		\STATE continue
		\ENDIF
		\STATE res$\leftarrow$ decompose(current, current.connectivity+1)//decomposition based on s-t-cuts
		\STATE current.connectivity $\leftarrow$ value of smallest encountered cut
		\IF {current.connectivity$\geq$alpha}
		\STATE continue
		\ENDIF
		\STATE childrenQueue $\leftarrow$ res
		\STATE //make sure that only subgraphs with higher connectivity are added as children
		\WHILE{childrenQueue not empty}
		\STATE c$\leftarrow$childrenQueue.pop()
		\STATE cRes$\leftarrow$decompose(c, current.connectivity+1)
		\STATE c.connectivity $\leftarrow$ value of smallest encountered cut
		\IF{decompose returned only one graph}
		\STATE current.children.add(cRes)
		\IF{cRes has connectivity smaller than alpha}
		\STATE q.push(cRes)
		\ENDIF
		\ELSE
		\STATE childrenQueue.add(cRes)
		\ENDIF
		\ENDWHILE
		\ENDWHILE
	\end{algorithmic}		
\end{algorithm}

\begin{algorithm}
	\caption{delComponents(del)}
	\label{delComponents}
	\begin{algorithmic}
		\STATE delInterEdges(del)
		\STATE root.connectivity=0
		\STATE root.children=\{\}
		\STATE updateDecomposition(0,1)
	\end{algorithmic}
\end{algorithm}


\subsection{Algorithm Explanations}
\begin{itemize}
	\item \cref{insertAndUpdate} calls the other routines as needed
	\item \cref{updateDecomposition} starts at the smallest subgraph containing the nodes a and b in the decomposition tree and computes a new decomposition of the subgraph. Specifically it uses the decomposition approach from \cite{Chang2013} to decompose one subgraph and then also computes the subgraphs with the next higher connectivity and recurses until the connectivity has reached alpha.
	\item updateMapping checks whether the alphaConnectedComponents were changed. If yes then it either collocates them if the resulting component is small enough or it adds the component to its return value. Then all the returned components are deleted, i.e. the edges connecting its nodes are deleted and the decomposition is recomputed
	\item this deletion is performed by \cref{delComponents}
\end{itemize}


\section{Evaluation}
% figures for the evaluation


%TODO give sources for parmetis and metis performance and results
%TODO add reference to algorithm description overview
For this evaluation we compare our implementation described in \cref{implDetSection} to a static algorithm available via Metis (METIS\_PartGraphRecursive) and an adaptive/dynamic algorithm (ParMETIS\_V3\_AdaptiveRepart) implemented in the ParMetis framework. Both frameworks are known to produce very good results and to be very fast.\nl
As input data we use several HPC traces, the nature of the data is described in more detail by Avin, Ghobadi, Griner and Schmid in \cite{Avin2019}.

All data sets contain 1024 different communication nodes and are limited to the first 300 000 requests. The value of $\alpha$ is set to be 6 and the algorithm was tasked to partition the nodes into 32 clusters of size 32 each. The dynamic algorithms were allowed to use augmentation with a factor of $2.1$, i.e. for the dynamic algorithms the maximum cluster capacities were $\lfloor32\cdot2.1\rfloor=67$.\nl
%TODO investigate capacity usage of static algorithm
To our knowledge it is not possible to specify hard limits for the capacities used by the static algorithm implemented in Metis and as a result there are some occurences where the algorithm exceeds the capacities that are allowed by a small amount.\nl\nl

\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	bar width=1.4cm,
	enlarge x limits=0.25,
	width=\textwidth,
	height=.5\textwidth,
	legend style={at={(0.5,-0.25)},
		anchor=north,legend columns=-1},
	symbolic x coords={A,B,C,D},
	xtick=data,
	xlabel={input set},
	nodes near coords,
	nodes near coords align={vertical},
	ymin=0,ymax=370000,
	ylabel={cost},
	]
	\addplot table[x=database,y=decomposition]{\totalcostplot};
	\addplot table[x=database,y=ParMetis]{\totalcostplot};
	\addplot table[x=database, y=Static]{\totalcostplot};
	\legend{Decomposition, ParMetis, Static}
	\end{axis}
	\end{tikzpicture}
	\caption{comparison of total cost}\label{totalCostPlot}
\end{figure}

\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	bar width=1.4cm,
	enlarge x limits=0.25,
	width=\textwidth,
	height=.5\textwidth,
	legend style={at={(0.5,-0.25)},
		anchor=north,legend columns=-1},
	symbolic x coords={A,B,C,D},
	xtick=data,
	xlabel={input set},
	nodes near coords,
	nodes near coords align={vertical},
	ymin=0,ymax=750000,
	ylabel={time in ms},
	]
	\addplot table[x=database,y=decomposition]{\runtimeplot};
	\addplot table[x=database,y=ParMetis]{\runtimeplot};
	\addplot table[x=database, y=Static]{\runtimeplot};
	\legend{Decomposition, ParMetis, Static}
	\end{axis}
	\end{tikzpicture}
	\caption{comparison of run time}\label{runTimePlot}
\end{figure}

We will first discuss the overall results of our experiments, i.e. we will describe the quality of the results (see \cref{totalCostPlot}) as well as the running time needed for each examined algorithm (see \cref{runTimePlot}).\nl
The static algorithm is shown to give the best results in the shortest time, but the low running was also to be expected as it is only called once as opposed to the 300 000 times the other algorithms need to decide whether they want to change their partitioning. The static algorithm also has knowledge of all requests and as a result is able to produce the best results. \nl
For data set A our decomposition algorithm beats the adaptive ParMetis algorithm by a significant amount of about one third of the cost of the latter while ParMetis is significantly faster. For data sets B and C ParMetis is shown to produce slightly better results within drastically less computation time. It is worth mentioning that ParMetis uses the ... graph description format that does not allow for easy adaptation on the fly and needed to be recomputed after every request during our tests. However, we chose not to include this in the running time calculations.\nl

\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	bar width=1.4cm,
	enlarge x limits=0.25,
	width=\textwidth,
	height=.5\textwidth,
	legend style={at={(0.5,-0.25)},
		anchor=north,legend columns=-1},
	symbolic x coords={A,B,C,D},
	xtick=data,
	xlabel={input set},
	nodes near coords,
	nodes near coords align={vertical},
	ymin=0,ymax=200000,
	ylabel={cost},
	]
	\addplot table[x=database,y=decomposition]{\commcostplot};
	\addplot table[x=database,y=ParMetis]{\commcostplot};
	\addplot table[x=database, y=Static]{\commcostplot};
	\legend{Decomposition, ParMetis, Static}
	\end{axis}
	\end{tikzpicture}
	\caption{comparison of communication cost}\label{commCostPlot}
\end{figure}

\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[
	ybar,
	bar width=1.4cm,
	enlarge x limits=0.25,
	width=\textwidth,
	height=.5\textwidth,
	legend style={at={(0.5,-0.25)},
		anchor=north,legend columns=-1},
	symbolic x coords={A,B,C,D},
	xtick=data,
	xlabel={input set},
	nodes near coords,
	nodes near coords align={vertical},
	ymin=0,ymax=370000,
	ylabel={cost},
	]
	\addplot table[x=database,y=decomposition]{\migcostplot};
	\addplot table[x=database,y=ParMetis]{\migcostplot};
	\addplot table[x=database, y=Static]{\migcostplot};
	\legend{Decomposition, ParMetis, Static}
	\end{axis}
	\end{tikzpicture}
	\caption{comparison of migration cost}\label{migCostPlot}
\end{figure}


In the next section we discuss the general distribution of the total costs of each algorithm to communication (see \cref{commCostPlot}) and migration costs (\cref{migCostPlot}).\nl
Both ParMetis as well as our decomposition algorithm produce significantly more migration cost than communication cost while the static algorithm predominantly pays for communication. This shows that the dynamic algorithms tend to migrate too much while the static implementation is restricted to only migrate once to a static configuration it finds suitable and as a result has to pay more for communication. This also shows that there is potential to refine the dynamic implementations in such a way that they produce more balanced, and hopefully also less, cost overall.

\clearpage %move references to the back
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{../../Dropbox/ThesisPapers/ThesisBibTex}                             

	
\end{document}
